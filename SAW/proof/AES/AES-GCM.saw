/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Block/AES.cry";
import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Authenticated/AES_256_GCM.cry";
import "../../spec/AES/X86.cry";
import "../../spec/AES/AES-GCM.cry";


enable_experimental;


m <- llvm_load_module "../../build/llvm/crypto/crypto_test.bc";


include "../common/helpers.saw";
include "../common/memory.saw";


/*
 * Verification parameters.
 */

// The number of bytes of input given to the update function.
let evp_cipher_update_len = 380;
// The total input length before the call to the update function.
let evp_cipher_update_gcm_len = 10;

/* 
 * The GCM implementation has multiple phases:
 * 1) A "bulk" encryption/decryption that operates on multiples of 6 blocks, and computes
 *    AES/CTR32 and GHASH in parallel using different functional units of the CPU. 
 * 2) An optimized AES/CTR32 implementation that processes the remaining blocks.
 * 3) An optimized GHASH implementation that processes the remaining blocks. 
 *
 * The code below calculates the correct input lengths for each subroutine.
 */

// If starting auth length is nonzero, bytes are collected and a single GCM_MUL is performed.
// So bulk GCM auth length is the starting length rounded up to the next multiple of 16.
let aesni_gcm_cipher_gcm_len = eval_size {| (evp_cipher_update_gcm_len + 15) / 16 * 16 |};
let gcm_len_diff = eval_size {| aesni_gcm_cipher_gcm_len - evp_cipher_update_gcm_len |};
let aesni_gcm_cipher_len = eval_size {| evp_cipher_update_len - (min gcm_len_diff evp_cipher_update_len) |};

// To get size for bulk encrypt operatin, round down to nearest multiple of 96 (min 288)
let evp_cipher_update_bulk_encrypt = eval_size {| max 288 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_encrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_encrypt |};
// Separate AES-CTR32 and GHASH encryption on the remaining whole blocks
// When there are no blocks left after the bulk operation, prove the CTR32 and GHASH
// operations correct for 1 block. The code is not entered, so this proof is never used,
// but it avoids complications related to proving these operations correct on 0 blocks.
let length_after_bulk_encrypt = eval_size {| aesni_gcm_cipher_len - bulk_encrypt_used * evp_cipher_update_bulk_encrypt |};
let whole_blocks_after_bulk_encrypt = eval_size {| max 1 (length_after_bulk_encrypt / 16) |};
let GHASH_length_blocks_encrypt = eval_size {| whole_blocks_after_bulk_encrypt * 16|};

// Bulk decrypt core also operates on 6 blocks, but the minimum is 6 blocks.
let evp_cipher_update_bulk_decrypt = eval_size {| max 96 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_decrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_decrypt |};
// Separate AES-CTR32 and GHASH decryption on the remaining whole blocks
// Limit to minimum of 1 block like in encrypt case.
let length_after_bulk_decrypt = eval_size {| aesni_gcm_cipher_len - bulk_decrypt_used * evp_cipher_update_bulk_decrypt |};
let whole_blocks_after_bulk_decrypt = eval_size {| max 1 (length_after_bulk_decrypt / 16) |};
let GHASH_length_blocks_decrypt = eval_size {| whole_blocks_after_bulk_decrypt * 16|};

let evp_cipher_final_gcm_len = eval_size {| evp_cipher_update_gcm_len + evp_cipher_update_len |};

// Log the calculated values to help find errors when the proof fails.
print (str_concat "evp_cipher_update_len=" (show evp_cipher_update_len));
print (str_concat "aesni_gcm_cipher_gcm_len=" (show aesni_gcm_cipher_gcm_len));
print (str_concat "aesni_gcm_cipher_len=" (show aesni_gcm_cipher_len));

print (str_concat "evp_cipher_update_bulk_encrypt=" (show evp_cipher_update_bulk_encrypt));
print (str_concat "bulk_encrypt_used=" (show bulk_encrypt_used));
print (str_concat "length_after_bulk_encrypt=" (show length_after_bulk_encrypt));
print (str_concat "whole_blocks_after_bulk_encrypt=" (show whole_blocks_after_bulk_encrypt));
print (str_concat "GHASH_length_blocks_encrypt=" (show GHASH_length_blocks_encrypt));

print (str_concat "evp_cipher_update_bulk_decrypt=" (show evp_cipher_update_bulk_decrypt));
print (str_concat "bulk_decrypt_used=" (show bulk_decrypt_used));
print (str_concat "length_after_bulk_decrypt=" (show length_after_bulk_decrypt));
print (str_concat "whole_blocks_after_bulk_decrypt=" (show whole_blocks_after_bulk_decrypt));
print (str_concat "GHASH_length_blocks_decrypt=" (show GHASH_length_blocks_decrypt));


include "goal-rewrites.saw";


let NID_aes_256_gcm = 901;
let aes_block_size = 1;
// the IV for AES-GCM consists of 12 32-bit integers
let aes_iv_len = 12;


/*
 * Architecture features for the AVX+shrd code path
 */
let {{ ia32cap = [0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff] : [4][32] }};


include "AES.saw";
include "GHASH.saw";
include "AESNI-GCM.saw";

/*
When verifying aes_hw_ctr32_encrypt_blocks, the binary analysis must locally
treat r11 as callee-preserved. This is necessary because this routine saves
the original stack pointer in r11 and then calls helper routines, preventing
the binary analysis from inferring that the return address is still on the stack
when the routine returns. The called helper routines do not modify r11.
*/

let aes_hw_ctr32_tactic = do {
  simplify (cryptol_ss ());
  simplify (addsimps slice_384_thms basic_ss);
  simplify (addsimps [cmp_sub_thm] empty_ss);
  goal_eval_unint ["AESRound", "AESFinalRound", "aesenc", "aesenclast"];
  simplify (addsimps add_xor_slice_thms basic_ss);
  simplify (addsimps aesenclast_thms basic_ss);
  w4_unint_yices ["AESRound", "AESFinalRound"];
};

add_x86_preserved_reg "r11";
aes_hw_ctr32_encrypt_blocks_encrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aes_hw_ctr32_encrypt_blocks"
  []
  true
  (aes_hw_ctr32_encrypt_blocks_spec whole_blocks_after_bulk_encrypt)
  aes_hw_ctr32_tactic;

aes_hw_ctr32_encrypt_blocks_decrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aes_hw_ctr32_encrypt_blocks"
  []
  true
  (aes_hw_ctr32_encrypt_blocks_spec whole_blocks_after_bulk_decrypt)
  aes_hw_ctr32_tactic;
default_x86_preserved_reg;


////////////////////////////////////////////////////////////////////////////////
// Specifications

let EVP_AES_GCM_CTX_PADDING = 8;
let EVP_AES_GCM_CTX_size = llvm_sizeof m (llvm_struct "struct.EVP_AES_GCM_CTX");
let ctx_size = eval_size {| EVP_AES_GCM_CTX_size + EVP_AES_GCM_CTX_PADDING |};


/*
 * Helpers for specifying the AES-GCM structs
 */
let EVP_CIPH_GCM_MODE = 0x6;
let EVP_CIPH_ALWAYS_CALL_INIT = 0x80;
let EVP_CIPH_CUSTOM_IV = 0x100;
let EVP_CIPH_CTRL_INIT = 0x200;
let EVP_CIPH_FLAG_CUSTOM_CIPHER = 0x400;
let EVP_CIPH_FLAG_AEAD_CIPHER = 0x800;
let EVP_CIPH_CUSTOM_COPY = 0x1000;

let points_to_evp_cipher_st ptr = do {
  crucible_points_to (crucible_elem ptr 0) (crucible_term {{ `NID_aes_256_gcm : [32] }});
  crucible_points_to (crucible_elem ptr 1) (crucible_term {{ `aes_block_size : [32] }});
  crucible_points_to (crucible_elem ptr 2) (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_elem ptr 3) (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_elem ptr 4) (crucible_term {{ `ctx_size : [32] }});
  let flags = eval_size {| EVP_CIPH_GCM_MODE + EVP_CIPH_CUSTOM_IV + EVP_CIPH_CUSTOM_COPY +
                           EVP_CIPH_FLAG_CUSTOM_CIPHER + EVP_CIPH_ALWAYS_CALL_INIT +
                           EVP_CIPH_CTRL_INIT + EVP_CIPH_FLAG_AEAD_CIPHER |};
  crucible_points_to (crucible_elem ptr 5) (crucible_term {{ `flags : [32] }});
  crucible_points_to (crucible_elem ptr 6) crucible_null;
  crucible_points_to (crucible_elem ptr 7) (crucible_global "aes_gcm_init_key");
  crucible_points_to (crucible_elem ptr 8) (crucible_global "aes_gcm_cipher");
  crucible_points_to (crucible_elem ptr 9) (crucible_global "aes_gcm_cleanup");
  crucible_points_to (crucible_elem ptr 10) (crucible_global "aes_gcm_ctrl");
};

let points_to_evp_cipher_ctx_st ptr cipher_ptr cipher_data_ptr enc = do {
  crucible_points_to (crucible_field ptr "cipher") cipher_ptr;
  crucible_points_to (crucible_field ptr "cipher_data") cipher_data_ptr;
  crucible_points_to (crucible_field ptr "key_len") (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_field ptr "encrypt") (crucible_term enc);
  crucible_points_to (crucible_field ptr "flags") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "buf_len") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "final_used") (crucible_term {{ 0 : [32] }});
};

let fresh_aes_gcm_ctx len = do {
  key <- fresh_aes_key_st;
  iv <- crucible_fresh_var "iv" (llvm_array aes_iv_len (llvm_int 8));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  return {{ { key = key, iv = iv, Xi = Xi, len = `len } : AES_GCM_Ctx }};
};

let points_to_gcm128_key_st ptr ctx = do {
  crucible_points_to_untyped (crucible_elem ptr 1) (crucible_term {{ get_Htable ctx }});
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_H ctx }});
  crucible_points_to (crucible_elem ptr 2) (crucible_global "gcm_gmult_avx");
  crucible_points_to (crucible_elem ptr 3) (crucible_global "gcm_ghash_avx");
  crucible_points_to (crucible_elem ptr 4) (crucible_global "aes_hw_encrypt");
  crucible_points_to (crucible_elem ptr 5) (crucible_term {{ 1 : [8] }});
};

let points_to_GCM128_CONTEXT ptr ctx mres = do {
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_Yi ctx }});
  if eval_bool {{ `mres == 0 }} then do {
    return ();
  } else do {
    crucible_points_to_untyped (crucible_elem ptr 1) (crucible_term {{ get_EKi ctx }});
  };
  crucible_points_to_untyped (crucible_elem ptr 2) (crucible_term {{ get_EK0 ctx }});
  crucible_points_to_untyped (crucible_elem ptr 3) (crucible_term {{ [(0 : [64]), ctx.len] }});
  crucible_points_to_untyped (crucible_elem ptr 4) (crucible_term {{ ctx.Xi }});
  points_to_gcm128_key_st (crucible_elem ptr 5) ctx;
  crucible_points_to (crucible_elem ptr 6) (crucible_term {{ `mres : [32] }});
  crucible_points_to (crucible_elem ptr 7) (crucible_term {{ 0 : [32] }});
};

let points_to_EVP_AES_GCM_CTX ptr ctx mres iv_set taglen = do {
  points_to_GCM128_CONTEXT (crucible_field ptr "gcm") ctx mres;
  points_to_aes_key_st (crucible_field ptr "ks") {{ ctx.key }};
  crucible_points_to (crucible_field ptr "key_set") (crucible_term {{ 1 : [32] }});
  crucible_points_to (crucible_field ptr "iv_set") (crucible_term iv_set);
  crucible_points_to (crucible_field ptr "ivlen") (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_field ptr "taglen") (crucible_term {{ `taglen : [32] }});
  crucible_points_to (crucible_field ptr "iv_gen") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "ctr") (crucible_global "aes_hw_ctr32_encrypt_blocks");
};


let aes_gcm_from_cipher_ctx_spec = do {
  ctx_ptr <- crucible_alloc_readonly (llvm_struct "struct.evp_cipher_ctx_st");
  cipher_data_ptr <- crucible_alloc_readonly_aligned 16 (llvm_struct "struct.EVP_AES_GCM_CTX");
  crucible_points_to (crucible_field ctx_ptr "cipher_data") cipher_data_ptr;

  crucible_execute_func [ctx_ptr];

  crucible_return cipher_data_ptr;
};


include "evp-function-specs.saw";


////////////////////////////////////////////////////////////////////////////////
// Proof commands

aes_gcm_from_cipher_ctx_ov <- crucible_llvm_unsafe_assume_spec
  m
  "aes_gcm_from_cipher_ctx"
  aes_gcm_from_cipher_ctx_spec;


llvm_verify m "aes_256_gcm_generic_init" [] true aes_256_gcm_generic_init_spec (w4_unint_yices []);


let evp_cipher_ovs =
  [ OPENSSL_malloc_ov
  , aes_gcm_from_cipher_ctx_ov
  , aes_hw_set_encrypt_key_ov
  , aes_hw_encrypt_ov
  , aes_hw_encrypt_in_place_ov
  , aes_hw_ctr32_encrypt_blocks_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_decrypt_ov
  , gcm_init_avx_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_encrypt_ov
  , gcm_ghash_avx_decrypt_ov
  , aesni_gcm_encrypt_ov
  , aesni_gcm_decrypt_ov
  ];


llvm_verify m "EVP_CipherInit_ex"
  evp_cipher_ovs
  true
  (EVP_CipherInit_ex_spec {{ 1 : [32] }})
  evp_cipher_tactic;

llvm_verify m "EVP_CipherInit_ex"
  evp_cipher_ovs
  true
  (EVP_CipherInit_ex_spec {{ 0 : [32] }})
  evp_cipher_tactic;


enable_what4_hash_consing;

llvm_verify m "EVP_EncryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 1 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
  evp_cipher_tactic;

llvm_verify m "EVP_DecryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 0 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
  evp_cipher_tactic;

disable_what4_hash_consing;


llvm_verify m "EVP_EncryptFinal_ex"
  evp_cipher_ovs
  true
  (EVP_EncryptFinal_ex_spec evp_cipher_final_gcm_len)
  evp_cipher_tactic;

llvm_verify m "EVP_DecryptFinal_ex"
  evp_cipher_ovs
  true
  (EVP_DecryptFinal_ex_spec evp_cipher_final_gcm_len)
  evp_cipher_tactic;

